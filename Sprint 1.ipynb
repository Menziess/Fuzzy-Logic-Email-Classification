{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint 1\n",
    "\n",
    "In de [eerste sprint](https://trello.com/b/RtxQyUB4/sprint-1-17-nov) werken we aan de volgende taken:\n",
    "\n",
    "1. [Email file reading methode schrijven](https://trello.com/c/L3ISKIgf/4-email-file-reading-methode-schrijven)\n",
    "2. [Cleaning methode schrijven](https://trello.com/c/QCD8JKLK/2-cleaning-methode-schrijven) + [Tokenize methode schrijven](https://trello.com/c/UrXLnJBi/3-tokenize-methode-schrijven)\n",
    "3. [Fuzzy logic model bedenken (inputs, MF's, outputs, rules enz..), Inspirational Paper Lezen](http://ijcsi.org/papers/IJCSI-10-3-2-48-58.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Email file reading methode schrijven\n",
    "\n",
    "Voor dit project staan resource files opgeslagen in de folder: *res*.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿Dear Mr. Lee,\n",
      "\n",
      "I write on behalf of my CEO, Ms. Douglas, of the Dutch cell phone company ‘‘cell.com’’.\n",
      "You’ve met Ms. Douglas at the Cebit Trade Fair in Hannover. She was highly satisfied with your inspirational inventions, especially the auto rechargeable cell phone. \n"
     ]
    }
   ],
   "source": [
    "# Global variable containing body of .txt file\n",
    "body = ''\n",
    "\n",
    "# Opening the example email file for reading\n",
    "# using a context manager (safe)\n",
    "with open('res/email.txt', 'r') as f:\n",
    "    size_to_read = 270\n",
    "    body = f.read(size_to_read)\n",
    "    \n",
    "print(body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Cleaning + tokenize methode schrijven\n",
    "\n",
    "#### 2.1 Splitting words based on whitespace or regex pattern\n",
    "\n",
    "Door een stuk text m.b.v. de regex \\[\\W+\\] te splitten worden de woorden al enigszins gecleaned. Maar woorden zoals \"you've\" worden opgesplitst in \"you\" en \"ve\", wat natuurlijk niet hoort.\n",
    "\n",
    "Door een stuk text op te splitsen aan de hand van whitespace, worden alle woorden intact gehouden, maar zit alle punctuatie er nog in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REGEX:\n",
      " ['', 'Dear', 'Mr', 'Lee', 'I', 'write', 'on', 'behalf', 'of', 'my', 'CEO', 'Ms', 'Douglas', 'of', 'the', 'Dutch', 'cell', 'phone', 'company', 'cell', 'com', 'You', 've', 'met', 'Ms', 'Douglas', 'at', 'the', 'Cebit', 'Trade', 'Fair', 'in', 'Hannover', 'She', 'was', 'highly', 'satisfied', 'with', 'your', 'inspirational', 'inventions', 'especially', 'the', 'auto', 'rechargeable', 'cell', 'phone', '']\n",
      "\n",
      "WHITESPACE:\n",
      " ['Dear', 'Mr.', 'Lee,', 'I', 'write', 'on', 'behalf', 'of', 'my', 'CEO,', 'Ms.', 'Douglas,', 'of', 'the', 'Dutch', 'cell', 'phone', 'company', '‘‘cell.com’’.', 'You’ve', 'met', 'Ms.', 'Douglas', 'at', 'the', 'Cebit', 'Trade', 'Fair', 'in', 'Hannover.', 'She', 'was', 'highly', 'satisfied', 'with', 'your', 'inspirational', 'inventions,', 'especially', 'the', 'auto', 'rechargeable', 'cell', 'phone.']\n"
     ]
    }
   ],
   "source": [
    "# split based on words only\n",
    "import re\n",
    "words = re.split(r'\\W+', body)\n",
    "\n",
    "print('REGEX:\\n', words[:50])\n",
    "print()\n",
    "\n",
    "# split into words by white space\n",
    "words = body.split()\n",
    "words[0] = words[0].replace('\\ufeff', '')\n",
    "\n",
    "print('WHITESPACE:\\n', words[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Removing punctuation\n",
    "\n",
    "Met de 'string' import kunnen we de punctuatie verwijderen, zonder dat we \"you've\" verliezen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dear', 'Mr', 'Lee', 'I', 'write', 'on', 'behalf', 'of', 'my', 'CEO', 'Ms', 'Douglas', 'of', 'the', 'Dutch', 'cell', 'phone', 'company', '‘‘cellcom’’', 'You’ve', 'met', 'Ms', 'Douglas', 'at', 'the', 'Cebit', 'Trade', 'Fair', 'in', 'Hannover', 'She', 'was', 'highly', 'satisfied', 'with', 'your', 'inspirational', 'inventions', 'especially', 'the', 'auto', 'rechargeable', 'cell', 'phone']\n"
     ]
    }
   ],
   "source": [
    "# remove punctuation from each word\n",
    "import string\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "words = [w.translate(table) for w in words]\n",
    "\n",
    "print(words[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Normalizing case\n",
    "\n",
    "Sommige woorden bevatten hoofdletters, sommige niet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dear', 'mr', 'lee', 'i', 'write', 'on', 'behalf', 'of', 'my', 'ceo', 'ms', 'douglas', 'of', 'the', 'dutch', 'cell', 'phone', 'company', '‘‘cellcom’’', 'you’ve', 'met', 'ms', 'douglas', 'at', 'the', 'cebit', 'trade', 'fair', 'in', 'hannover', 'she', 'was', 'highly', 'satisfied', 'with', 'your', 'inspirational', 'inventions', 'especially', 'the', 'auto', 'rechargeable', 'cell', 'phone']\n"
     ]
    }
   ],
   "source": [
    "# convert to lower case\n",
    "words = [word.lower() for word in words]\n",
    "print(words[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4. Filtering stop words & stemming\n",
    "\n",
    "Een alternatieve manier om tekst te cleanen is het gebruiken van een package zoals NLTK.\n",
    "\n",
    "Om nltk te installeren, gebruik pip of pip3: *'sudo pip3 install -U nltk'*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/stefan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/stefan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "['mr', 'lee', 'write', 'behalf', 'ceo', 'ms', 'dougla', 'dutch', 'cell', 'phone', 'compani', 'cellcom', 'met', 'ms', 'dougla', 'cebit', 'trade', 'fair', 'hannov', 'highli', 'satisfi', 'inspir', 'invent', 'especi', 'auto', 'recharg', 'cell', 'phone']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Tokenize\n",
    "tokens = word_tokenize(body)\n",
    "# print('TOKENS:\\n', tokens[:100])\n",
    "\n",
    "# Convert to lower case\n",
    "tokens = [w.lower() for w in tokens]\n",
    "\n",
    "# Remove punctuation from each word\n",
    "import string\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "stripped = [w.translate(table) for w in tokens]\n",
    "\n",
    "# Remove remaining tokens that are not alphabetic\n",
    "words = [word for word in stripped if word.isalpha()]\n",
    "\n",
    "# Filter out stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "words = [w for w in words if not w in stop_words]\n",
    "\n",
    "# Stemming of words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "words = [porter.stem(word) for word in words]\n",
    "\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Fuzzy logic model bedenken (inputs, MF's, outputs, rules enz..), Inspirational Paper Lezen\n",
    "\n",
    "Paper: [http://ijcsi.org/papers/IJCSI-10-3-2-48-58.pdf](http://ijcsi.org/papers/IJCSI-10-3-2-48-58.pdf)\n",
    "\n",
    "In hoofdstuk 3.1 Fuzzy Classification Module, wordt uitgelegd hoe spam woorden worden beoordeeld en gevoed aan een FLS. Net als in het ontwerp van ons project, worden woorden gecleaned, getokenized en vervolgens beoordeeld. \n",
    "\n",
    "Voor elke beoordeling van ieder woord kan een vector worden gemaakt met features (bijvoorbeeld: \"overlap_dept1\", \"sentiment\", \"technische_term\", enz...), zodat alle woorden kunnen worden meeggeven als een vector van feature vectors:\n",
    "\n",
    "$words = [\\overrightarrow{F_1}, \\overrightarrow{F_2}, ..., \\overrightarrow{F_n}]$\n",
    "\n",
    "Elke feature kan als input aan het FLS worden meegegeven, zo dat voor ieder woord een \"ranking\" wordt berekend:\n",
    "\n",
    "$A = \\{f, \\mu_A(f)$ | $ f \\in F_1 \\}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
