{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint 2\n",
    "\n",
    "In de [tweede sprint](https://trello.com/b/B68ygqCl/sprint-2-24-nov) werken we aan de volgende taken:\n",
    "\n",
    "1. [Classificatie Test Schrijven](https://trello.com/c/1igFlI8G/2-classificatie-test-schrijven)\n",
    "2. [Fuzzy Logic Toolbox implementeren](https://trello.com/c/TUYYbdPU/1-fuzzy-logic-toolbox-implementeren)\n",
    "3. [Fuzzy logic model bedenken (inputs, MF's, outputs, rules enz...)](https://trello.com/c/Y6rP7daC/4-fuzzy-logic-model-bedenken-inputs-mfs-outputs-rules-enz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning + Tokenizing\n",
    "\n",
    "In [Sprint 1](http://localhost:8888/notebooks/Sprint%201.ipynb) hebben we cleaning methodes toegepast die we hier gebruiken om emails en woordenlijsten te tokenizen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/stefan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/stefan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleans and tokenizes document body\n",
    "def tokenize(body):\n",
    "    tokens = word_tokenize(body)\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    porter = PorterStemmer()\n",
    "    words = [porter.stem(word) for word in words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Classificatie Test Schrijven\n",
    "\n",
    "Er moet een systeem geschreven worden die de basis vormt voor het classificeren van meerdere emails, waarbij een overzicht van de classificatie en analyse van de correct en fout geclassificeerde emails wordt getoond."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized email:\n",
      " ['mr', 'lee', 'write', 'behalf', 'ceo', 'ms', 'dougla', 'dutch', 'cell', 'phone', 'compani', 'cellcom', 'met', 'ms', 'dougla', 'cebit', 'trade', 'fair', 'hannov', 'highli', 'satisfi', 'inspir', 'invent', 'especi', 'auto', 'recharg', 'cell', 'phone']\n"
     ]
    }
   ],
   "source": [
    "# Opening the example email file for reading\n",
    "# using a context manager (safe)\n",
    "with open('res/email.txt', 'r') as f:\n",
    "    size_to_read = 270\n",
    "    body = f.read(size_to_read)\n",
    "    print('Tokenized email:\\n', tokenize(body))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Fuzzy Logic Toolbox implementeren\n",
    "\n",
    "The fundamental classes and functions of fuzzy logic.\n",
    "\n",
    "* Variables\n",
    "* Rules\n",
    "* MF's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Fuzzy logic model bedenken (inputs, MF's, outputs, rules enz..), Inspirational Paper Lezen\n",
    "\n",
    "Paper: [http://ijcsi.org/papers/IJCSI-10-3-2-48-58.pdf](http://ijcsi.org/papers/IJCSI-10-3-2-48-58.pdf)\n",
    "\n",
    "In hoofdstuk 3.1 Fuzzy Classification Module, wordt uitgelegd hoe spam woorden worden beoordeeld en gevoed aan een FLS. Net als in het ontwerp van ons project, worden woorden gecleaned, getokenized en vervolgens beoordeeld. \n",
    "\n",
    "Voor elke beoordeling van ieder woord kan een vector worden gemaakt met features (bijvoorbeeld: \"overlap_dept1\", \"sentiment\", \"technische_term\", enz...), zodat alle woorden kunnen worden meeggeven als een vector van feature vectors:\n",
    "\n",
    "$words = [\\overrightarrow{F_1}, \\overrightarrow{F_2}, ..., \\overrightarrow{F_n}]$\n",
    "\n",
    "Elke feature kan als input aan het FLS worden meegegeven, zo dat voor ieder woord een \"ranking\" wordt berekend:\n",
    "\n",
    "$A = \\{f, \\mu_A(f)$ | $ f \\in F_1 \\}$\n",
    "\n",
    "\n",
    "![Schematic representation of a possible solution](https://trello-attachments.s3.amazonaws.com/5a0bfeca872850a499b18adf/5a0bfdf659a1523be5712ed8/dfac7bc5ecc1332c92f28a0d7ee76663/image.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
