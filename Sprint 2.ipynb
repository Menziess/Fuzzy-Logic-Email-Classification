{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint 2\n",
    "\n",
    "In de [tweede sprint](https://trello.com/b/B68ygqCl/sprint-2-24-nov) werken we aan de volgende taken:\n",
    "\n",
    "1. [Classificatie Test Schrijven](https://trello.com/c/1igFlI8G/2-classificatie-test-schrijven)\n",
    "2. [Fuzzy Logic Toolbox implementeren](https://trello.com/c/TUYYbdPU/1-fuzzy-logic-toolbox-implementeren)\n",
    "3. [Fuzzy logic model bedenken (inputs, MF's, outputs, rules enz...)](https://trello.com/c/Y6rP7daC/4-fuzzy-logic-model-bedenken-inputs-mfs-outputs-rules-enz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning + Tokenizing\n",
    "\n",
    "In [Sprint 1](http://localhost:8888/notebooks/Sprint%201.ipynb) hebben we cleaning methodes toegepast die we hier gebruiken om emails en woordenlijsten te tokenizen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/stefan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/stefan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(body):\n",
    "    tokens = word_tokenize(body)\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    porter = PorterStemmer()\n",
    "    words = [porter.stem(word) for word in words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_txt(file):\n",
    "    with open(file, 'r') as t:\n",
    "        body = t.read()\n",
    "        return tokenize(body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(file):\n",
    "    with open(file, 'r') as c:\n",
    "        reader = csv.reader(c, delimiter=',')\n",
    "        for row in reader:\n",
    "            return row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Classificatie Test Schrijven\n",
    "\n",
    "Er moet een systeem geschreven worden die de basis vormt voor het classificeren van meerdere emails, waarbij een overzicht van de classificatie en analyse van de correct en fout geclassificeerde emails wordt getoond.\n",
    "\n",
    "#### 1.1 Generating Word List\n",
    "\n",
    "Deze word_list wordt gebruikt om all niet relevante woorden uit de texten te filteren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_csv_from_array(filename, array):\n",
    "    with open(\"res/\" + filename + \".csv\", 'w', newline='') as c:\n",
    "        writer = csv.writer(c, delimiter=',')\n",
    "        writer.writerow(array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Intersection\n",
    "\n",
    "Filtert alle elementen uit array1 die niet in array2 zitten eruit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection(array1, array2):\n",
    "    \"\"\"Returns a generator, use next(generator)\"\"\"\n",
    "    return (i for i in array1 if i in array2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Corpus\n",
    "\n",
    "Gebruikt [Counter](https://docs.python.org/3/library/collections.html) om alle woorden te tellen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter()\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "email     = read_txt('res/email.txt')\n",
    "word_list = read_csv('res/word_list.csv')\n",
    "\n",
    "corpus = Counter(intersection(email, word_list))\n",
    "\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Score Calculation\n",
    "\n",
    "Elk woord zal 1 of meer features krijgen, gebaseerd op het feit dat het bevat is in bekende sets van woorden van een bepaalde klasse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wikileak' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-74b8c4e9cbcf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# een evental aan karakters hebben\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0memail\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0mread_txt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'res/email.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mword_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwikileak\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mgenerate_csv_from_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word_list\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wikileak' is not defined"
     ]
    }
   ],
   "source": [
    "# Maakt een word_list die simpelweg alle woorden bevat die \n",
    "# een evental aan karakters hebben\n",
    "email     = read_txt('res/email.txt')\n",
    "word_list = [x for x in sorted(set(email)) if len(x) % 2 == 0]\n",
    "generate_csv_from_array(\"word_list\", word_list)\n",
    "\n",
    "# Maakt een feature_list met woorden die met een c beginnen\n",
    "starts_with_c = [x for x in word_list if x[0] == 'c']\n",
    "generate_csv_from_array(\"starts_with_c\", starts_with_c)\n",
    "\n",
    "# Maakt een feature_list met woorden die met een m beginnen\n",
    "starts_with_m = [x for x in word_list if x[0] == 'm']\n",
    "generate_csv_from_array(\"starts_with_m\", starts_with_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Fuzzy Logic Toolbox implementeren\n",
    "\n",
    "The fundamental classes and functions of fuzzy logic.\n",
    "\n",
    "* Variables\n",
    "* Rules\n",
    "* MF's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jim Kamans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FuzStep 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Fuzzy logic model bedenken (inputs, MF's, outputs, rules enz..), Inspirational Paper Lezen\n",
    "\n",
    "Paper: [http://ijcsi.org/papers/IJCSI-10-3-2-48-58.pdf](http://ijcsi.org/papers/IJCSI-10-3-2-48-58.pdf)\n",
    "\n",
    "In hoofdstuk 3.1 Fuzzy Classification Module, wordt uitgelegd hoe spam woorden worden beoordeeld en gevoed aan een FLS. Net als in het ontwerp van ons project, worden woorden gecleaned, getokenized en vervolgens beoordeeld. \n",
    "\n",
    "Voor elke beoordeling van ieder woord kan een vector worden gemaakt met features (bijvoorbeeld: \"technisch\", \"media\", \"klacht\", \"\", enz...), zodat alle woorden kunnen worden meeggeven als een vector van feature vectors:\n",
    "\n",
    "$X = [W, \\overrightarrow{F_1}, \\overrightarrow{F_2}, ..., \\overrightarrow{F_n}]$\n",
    "\n",
    "$x = [w, f_1, f_2, f_3]$\n",
    "\n",
    "<p align=\"center\"><img width=\"625\" src=\"https://i.imgur.com/HYQRXDK.jpg\"></p>\n",
    "\n",
    "Elke feature kan als input aan het FLS worden meegegeven, zo dat voor ieder woord een \"ranking\" wordt berekend:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
